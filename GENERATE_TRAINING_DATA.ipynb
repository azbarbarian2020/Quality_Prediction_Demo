{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "2ryvp32ijjivglm67u3z",
   "authorId": "2028723234080",
   "authorName": "ADMIN",
   "authorEmail": "jason.drew@snowflake.com",
   "sessionId": "86cd3f36-1943-4139-b69e-2c82a675ec51",
   "lastEditTime": 1740759306415
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366e5f21-2480-4f3f-93ea-fe2d21273779",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "Step 1:  Import needed packages set active session"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false
   },
   "source": "# Import python packages, added numpy \nimport streamlit as st\nimport pandas as pd\nimport numpy as np\nimport random\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "513a3256-761a-4b76-9cdc-f72cfaa33a8e",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "Step 2:  Generate training data to Pandas DataFrame"
  },
  {
   "cell_type": "code",
   "id": "db43db59-3cc6-4f17-8620-74495755c74e",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false
   },
   "outputs": [],
   "source": "def generate_batch_metrics(batch_id, machine_name, base_quality=0.98, is_outlier=False):\n    \"\"\"Generate metrics for a specific machine in a batch\"\"\"\n    params = {\n        'feed_rate': {\n            'nominal_min': 100, 'nominal_max': 120,\n            'total_min': 80, 'total_max': 140,\n            'impact_weight': 0.3,\n            'target_abnormal_pct': random.uniform(5, 25)\n        },\n        'vibration': {\n            'nominal_min': 0.1, 'nominal_max': 0.3,\n            'total_min': 0.05, 'total_max': 0.8,\n            'impact_weight': 0.25,\n            'target_abnormal_pct': random.uniform(5, 25)\n        },\n        'spindle_speed': {\n            'nominal_min': 2800, 'nominal_max': 3200,\n            'total_min': 2500, 'total_max': 3500,\n            'impact_weight': 0.25,\n            'target_abnormal_pct': random.uniform(5, 25)\n        },\n        'tool_wear': {\n            'impact_weight': 0.2\n        }\n    }\n    \n    # Add machine-specific variations\n    if machine_name == \"Bjorn\":\n        params['feed_rate']['target_abnormal_pct'] *= 1.1  # 10% more feed rate issues\n    elif machine_name == \"Ragnar\":\n        params['vibration']['target_abnormal_pct'] *= 1.15  # 15% more vibration issues\n    elif machine_name == \"Magnus\":\n        params['spindle_speed']['target_abnormal_pct'] *= 1.05  # 5% more spindle speed issues\n    elif machine_name == \"Harald\":\n        # Harald is generally more reliable but wears tools faster\n        for sensor in ['feed_rate', 'vibration', 'spindle_speed']:\n            params[sensor]['target_abnormal_pct'] *= 0.9  # 10% fewer issues overall\n    \n    metrics = {\n        'BATCH_ID': batch_id,\n        'MACHINE_NAME': machine_name\n    }\n    quality_impact = 0\n    \n    # Generate outlier data if requested\n    if is_outlier:\n        # Pick a random sensor to have extreme values\n        outlier_sensor = random.choice(['feed_rate', 'vibration', 'spindle_speed'])\n        params[outlier_sensor]['target_abnormal_pct'] = random.uniform(60, 80)\n        \n        # For tool wear, potential extreme value\n        if random.random() > 0.5:\n            tool_wear = random.uniform(80, 95)  # Extremely worn tool\n        else:\n            tool_wear = random.uniform(30, 70)  # Normal range\n    else:\n        # Harald wears tools faster\n        if machine_name == \"Harald\":\n            tool_wear = random.uniform(40, 75)  # Higher tool wear\n        else:\n            tool_wear = random.uniform(30, 70)  # Normal range\n    \n    for sensor in ['feed_rate', 'vibration', 'spindle_speed']:\n        p = params[sensor]\n        abnormal_pct = p['target_abnormal_pct']\n        metrics[f'{sensor.upper()}_ABNORMAL_PCT'] = abnormal_pct\n        \n        nominal_avg = (p['nominal_min'] + p['nominal_max']) / 2\n        total_range = p['total_max'] - p['total_min']\n        \n        # Add more deviation for outliers\n        if is_outlier and sensor == outlier_sensor:\n            bias = 1 if random.random() > 0.5 else -1\n            avg_deviation = total_range * 0.3 * bias  # More extreme deviation\n        elif abnormal_pct > 15:\n            bias = 1 if random.random() > 0.5 else -1\n            avg_deviation = total_range * 0.15 * bias\n        else:\n            avg_deviation = total_range * 0.05 * random.uniform(-1, 1)\n        \n        metrics[f'{sensor.upper()}_AVG'] = nominal_avg + avg_deviation\n        severity = (abnormal_pct / 100) * (abs(avg_deviation) / total_range)\n        quality_impact += severity * p['impact_weight']\n    \n    metrics['TOOL_WEAR'] = tool_wear\n    \n    if tool_wear > 50:\n        wear_impact = ((tool_wear - 50) / 50) ** 2 * params['tool_wear']['impact_weight']\n        quality_impact += wear_impact\n    \n    # Calculate quality yield\n    if is_outlier and random.random() < 0.3:\n        # Sometimes outliers have unexpectedly good quality\n        metrics['QUALITY_YIELD'] = random.uniform(0.92, 0.96)\n    else:\n        # Add slight machine-specific offsets to base quality\n        machine_base_quality = base_quality\n        if machine_name == \"Bjorn\":\n            machine_base_quality -= 0.005  # Slightly lower base quality\n        elif machine_name == \"Harald\":\n            machine_base_quality += 0.005  # Slightly higher base quality\n            \n        metrics['QUALITY_YIELD'] = max(0, min(1, machine_base_quality - quality_impact))\n    \n    # Add quality category classification\n    quality_value = metrics['QUALITY_YIELD']\n    if quality_value >= 0.95:\n        metrics['QUALITY_CATEGORY'] = 'Excellent'\n    elif quality_value >= 0.92:\n        metrics['QUALITY_CATEGORY'] = 'Good'\n    elif quality_value >= 0.88:\n        metrics['QUALITY_CATEGORY'] = 'Acceptable'\n    else:\n        metrics['QUALITY_CATEGORY'] = 'Poor'\n    \n    return metrics\n\ndef generate_training_dataset(n_batches=100, n_outliers=2):\n    \"\"\"Generate training dataset with n_batches across Viking machines including some outliers\"\"\"\n    \n    training_data = []\n    machine_names = [\"Bjorn\", \"Ragnar\", \"Magnus\", \"Harald\"]\n    \n    # Generate regular batches for each machine\n    for i in range(n_batches - n_outliers):\n        batch_id = f\"BATCH_{i+1:03d}\"\n        for machine_name in machine_names:\n            batch_metrics = generate_batch_metrics(batch_id, machine_name)\n            training_data.append(batch_metrics)\n    \n    # Generate outlier batches (randomly distributed among machines)\n    for i in range(n_outliers):\n        batch_id = f\"BATCH_{n_batches-n_outliers+i+1:03d}\"\n        outlier_machine = random.choice(machine_names)\n        \n        # Generate normal data for non-outlier machines\n        for machine_name in machine_names:\n            if machine_name == outlier_machine:\n                batch_metrics = generate_batch_metrics(batch_id, machine_name, is_outlier=True)\n            else:\n                batch_metrics = generate_batch_metrics(batch_id, machine_name)\n            training_data.append(batch_metrics)\n    \n    # Create DataFrame\n    df = pd.DataFrame(training_data)\n    return df\n\n# Create the dataframe\ndf = generate_training_dataset(100, n_outliers=2)\n\n# Display sample with both continuous and categorical targets\ndisplay_columns = [\n    'BATCH_ID',\n    'MACHINE_NAME',\n    'FEED_RATE_ABNORMAL_PCT', 'FEED_RATE_AVG',\n    'VIBRATION_ABNORMAL_PCT', 'VIBRATION_AVG',\n    'SPINDLE_SPEED_ABNORMAL_PCT', 'SPINDLE_SPEED_AVG',\n    'TOOL_WEAR',\n    'QUALITY_YIELD',\n    'QUALITY_CATEGORY'\n]\n\n# Display sample data\nprint(\"\\nSample of training data:\")\nprint(df[display_columns].head())\n\n# Show quality distribution\nprint(\"\\nQuality Category Distribution:\")\nprint(df['QUALITY_CATEGORY'].value_counts())\n\n# Show quality by machine\nprint(\"\\nAverage Quality by Machine:\")\nprint(df.groupby('MACHINE_NAME')['QUALITY_YIELD'].mean())\n\n# Show extreme values (potential outliers)\nprint(\"\\nPotential Outliers:\")\noutliers = df[\n    (df['FEED_RATE_ABNORMAL_PCT'] > 50) | \n    (df['VIBRATION_ABNORMAL_PCT'] > 50) | \n    (df['SPINDLE_SPEED_ABNORMAL_PCT'] > 50) |\n    (df['TOOL_WEAR'] > 80)\n]\nprint(outliers[display_columns].head())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cadfce73-6eb2-407f-8178-3d9d0a802043",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "Step 3:  Convert Pandas Dataframe to Snowpark Dataframe, then save to Snowflake Table"
  },
  {
   "cell_type": "code",
   "id": "1a8499c9-3df7-48a2-bbc7-aa004835203b",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": false
   },
   "outputs": [],
   "source": "snowpark_df = session.create_dataframe(df)\n# Write Snowpark DataFrame to Snowflake table\ntable_name = \"demo_db.streaming.train_table\"\nsnowpark_df.write.mode(\"overwrite\").save_as_table(table_name)",
   "execution_count": null
  }
 ]
}